Cheatsheet â€” Lancer, rÃ©gÃ©nÃ©rer, dÃ©panner
1) PrÃ©parer lâ€™environnement
# Se placer dans le projet + activer le venv
cd "C:\Users\Maintenant PrÃªt\Desktop\Projet_credit_scoring"
.\.venv\Scripts\Activate.ps1

# Mettre Ã  jour lâ€™outillage + installer les deps figÃ©es
python -m pip install --upgrade pip setuptools wheel
pip install -r requirements.txt --no-cache-dir

# (optionnel) Variables dâ€™env â€“ utile si lâ€™API nâ€™est pas en local
$env:API_URL = "http://127.0.0.1:8000"


ðŸ‘‰ VÃ©rifier Python : python -V doit renvoyer 3.10.x.

2) (Re)gÃ©nÃ©rer les artefacts du dashboard
# Stats population + features dÃ©rivÃ©es (Ã©crit artifacts/ref_stats.json)
python make_ref_stats.py

# Importances globales (Ã©crit artifacts/global_importance.csv + interpretability_summary.json)
python make_global_importance.py


make_ref_stats.py lit data\application_train.csv (chemin dÃ©jÃ  configurÃ© dans le script).
Si tu changes dâ€™emplacement, adapte TRAIN_CSV dans le script.

3) DÃ©marrer lâ€™API de scoring (FastAPI)
python -m uvicorn api:app --reload


Smoke tests rapides :

Invoke-WebRequest http://127.0.0.1:8000/ -UseBasicParsing        # status 200, montre chosen_model & threshold
Invoke-WebRequest http://127.0.0.1:8000/expected_columns -UseBasicParsing  # doit renvoyer count > 0


ðŸ§  Endpoints principaux :

GET / : statut & mÃ©ta (modÃ¨le, seuilâ€¦)

GET /expected_columns : colonnes dâ€™entrÃ©e requises

POST /predict : {"data": {...}} â†’ proba & dÃ©cision

POST /predict_proba_batch : {"records":[{...},{...}]} â†’ liste de rÃ©sultats

4) Lancer le dashboard (Streamlit)
# Toujours avec le Python du venv :
python -m streamlit run streamlit_app.py


Dans lâ€™UI :

VÃ©rifie lâ€™encart Ã‰tat de lâ€™API (doit Ãªtre âœ…).

Renseigne revenu/crÃ©dit/Ã¢ge â†’ clique Ã‰valuer ce dossier.

Sections Comparaison population et Variables influentes sâ€™activent si les artefacts sont prÃ©sents.

Changer lâ€™URL de lâ€™API : dans la sidebar, champ â€œAPI URLâ€ (le cache est gÃ©rÃ©, mise Ã  jour immÃ©diate).

5) Mode batch (CSV)

Dans la sidebar du dashboard â†’ PrÃ©dictions en lot (CSV) :

envoie un CSV avec les mÃªmes colonnes que /expected_columns (les colonnes manquantes sont complÃ©tÃ©es Ã  None).

la rÃ©ponse sâ€™affiche dans un tableau (proba + dÃ©cision).

6) Git â€” sauvegarder tes modifs
# Toujours Ã  la racine du projet & venv activÃ©
git checkout -b feat/dashboard-v1

# Ajouter les fichiers utiles (pas .venv/ ni data/)
git add api.py streamlit_app.py make_ref_stats.py make_global_importance.py `
        requirements.txt .gitignore .gitattributes `
        artifacts/metadata.json artifacts/ref_stats.json artifacts/global_importance.csv

git commit -m "feat: dashboard Streamlit + API robustifiÃ©e + artefacts utiles"
git push -u origin feat/dashboard-v1


Ouvre la PR : https://github.com/vanessa0911/projet_credit_scoring/compare

Gros fichiers : les modÃ¨les *.joblib et *.npy sont suivis via Git-LFS (dÃ©jÃ  rÃ©glÃ© par .gitattributes).
Si GitHub refuse un push (>100 MB), exÃ©cute :

git lfs install
git lfs migrate import --include="artifacts/*.joblib,artifacts/*.npy"
git push --force-with-lease

7) DÃ©pannage express

Streamlit/Pandas/NumPy/Arrow : si tu vois _ARRAY_API ou multiarray â†’ tes versions sont maintenant pinnÃ©es dans requirements.txt. RÃ©installe :

pip uninstall -y pyarrow numpy pandas
pip install -r requirements.txt --no-cache-dir


Port dÃ©jÃ  utilisÃ© :
netstat -ano | findstr :8000 â†’ rÃ©cupÃ¨re le PID â†’ taskkill /PID <PID> /F

Cache Streamlit :
streamlit cache clear

API non joignable : assure-toi que uvicorn tourne, et que API_URL dans Streamlit pointe bien vers http://127.0.0.1:8000.

expected_columns vide : vÃ©rifie artifacts/metadata.json (clÃ© expected_input_columns) ou laisse lâ€™API dÃ©duire depuis le modÃ¨le; relance uvicorn.

8) Structure du repo (rappel)
Projet_credit_scoring/
â”œâ”€ api.py                     # API FastAPI
â”œâ”€ streamlit_app.py           # UI Streamlit
â”œâ”€ make_ref_stats.py          # Stats population (ref_stats.json)
â”œâ”€ make_global_importance.py  # Importances (global_importance.csv)
â”œâ”€ requirements.txt
â”œâ”€ .gitignore / .gitattributes (LFS)
â”œâ”€ data/                      # (ignorÃ© par Git)
â””â”€ artifacts/
   â”œâ”€ metadata.json
   â”œâ”€ model_*.joblib         # (via Git-LFS)
   â”œâ”€ feature_names.npy      # (via Git-LFS)
   â”œâ”€ ref_stats.json
   â”œâ”€ global_importance.csv
   â””â”€ interpretability_summary.json
